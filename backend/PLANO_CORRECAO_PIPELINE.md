# üîß PLANO DE CORRE√á√ÉO DA PIPELINE

**Data:** 2025-10-20
**Status:** üî¥ CR√çTICO - Requer a√ß√£o imediata
**Accuracy Atual:** 34.3% (Meta: 60%+)

---

## üö® PROBLEMAS CR√çTICOS IDENTIFICADOS

### üìä Resumo Executivo

| Problema | Impacto | Prioridade | Causa Raiz |
|----------|---------|------------|------------|
| **#1: ML prevendo DRAW demais** | üî¥ CR√çTICO | P0 | Gerador cria predictions para TODOS markets com edge > 0 |
| **#2: 63.4% predictions n√£o validadas** | üü° ALTO | P1 | Jogos futuros + falta de limpeza |
| **#3: Confidence scores descalibrados** | üü° ALTO | P1 | Usando probabilidade direta sem calibra√ß√£o |

---

## üî• PROBLEMA #1: ML PREVENDO DRAW DEMAIS

### Dados do Problema

**Distribui√ß√£o REAL dos resultados:**
- Home Win: 44.4%
- Draw: 22.5%
- Away Win: 33.1%

**Distribui√ß√£o das PREDICTIONS do ML:**
- HOME_WIN: 12.5%
- DRAW: 36.6% ‚ö†Ô∏è (deveria ser ~22%)
- AWAY_WIN: 13.2%
- BTTS_NO: 36.4%

**Accuracy por Outcome:**
- HOME_WIN: 28.3%
- DRAW: 17.3% üî¥ P√âSSIMO!
- AWAY_WIN: 26.1%
- BTTS_NO: 55.7% ‚úÖ BOM
- 1X2 (antigo): 48.6% ‚úÖ OK

### Causa Raiz

**Arquivo:** `app/services/ml_prediction_generator.py` (linha 48-57, 439-444)

```python
MARKETS = [
    'HOME_WIN', 'DRAW', 'AWAY_WIN',
    'BTTS_YES', 'BTTS_NO',
    'OVER_2_5', 'UNDER_2_5',
]

# Filtro atual (linha 439-444):
if not (is_value_bet or probability > 0.60 or edge > 0):
    return None
```

**O problema:**
1. O gerador itera sobre TODOS os 7 markets
2. Para CADA market, se tiver edge > 0 (qualquer edge positivo), cria prediction
3. DRAW geralmente tem odds ~3.0-3.5 (altas)
4. Com probabilidade Poisson de 25-30%, facilmente tem edge > 0
5. Resultado: MUITAS predictions de DRAW s√£o geradas
6. Mas DRAW √© o outcome mais dif√≠cil de prever!

### Solu√ß√µes Propostas

#### Solu√ß√£o 1: Selecionar MELHOR outcome por jogo (RECOMENDADO)

```python
def _select_best_1x2_outcome(self, match: Match) -> Optional[str]:
    """
    Analisa os 3 outcomes (HOME_WIN, DRAW, AWAY_WIN) e retorna apenas o MELHOR

    Crit√©rios:
    1. Maior probabilidade
    2. Se probabilidades pr√≥ximas (diff < 10%), escolher por edge
    3. DRAW s√≥ se probabilidade > 35% OU edge > 20%
    """
    poisson_analysis = self._get_poisson_analysis(match)

    outcomes = {
        'HOME_WIN': {
            'prob': poisson_analysis.probabilities['HOME_WIN'],
            'edge': self._calculate_edge(match, 'HOME_WIN')
        },
        'DRAW': {
            'prob': poisson_analysis.probabilities['DRAW'],
            'edge': self._calculate_edge(match, 'DRAW')
        },
        'AWAY_WIN': {
            'prob': poisson_analysis.probabilities['AWAY_WIN'],
            'edge': self._calculate_edge(match, 'AWAY_WIN')
        }
    }

    # FILTRO ESPECIAL PARA DRAW: Exigir mais
    if outcomes['DRAW']['prob'] < 0.35 and outcomes['DRAW']['edge'] < 20:
        outcomes['DRAW']['prob'] = 0  # Remover DRAW da competi√ß√£o

    # Selecionar outcome com maior probabilidade
    best_outcome = max(outcomes.items(), key=lambda x: x[1]['prob'])

    # Se probabilidade muito baixa, n√£o gerar prediction
    if best_outcome[1]['prob'] < 0.30:
        return None

    return best_outcome[0]
```

**Impacto estimado:**
- ‚úÖ Reduzir predictions DRAW de 36.6% para ~20-25%
- ‚úÖ Aumentar accuracy DRAW de 17.3% para ~30-35%
- ‚úÖ Aumentar accuracy geral de 34.3% para ~45-50%

#### Solu√ß√£o 2: Ajustar thresholds por outcome

```python
# Thresholds espec√≠ficos por market
MARKET_THRESHOLDS = {
    'HOME_WIN': {'min_prob': 0.30, 'min_edge': 0},
    'DRAW': {'min_prob': 0.35, 'min_edge': 15},  # DRAW mais exigente
    'AWAY_WIN': {'min_prob': 0.30, 'min_edge': 0},
    'BTTS_YES': {'min_prob': 0.45, 'min_edge': 5},
    'BTTS_NO': {'min_prob': 0.50, 'min_edge': 5},
    'OVER_2_5': {'min_prob': 0.50, 'min_edge': 5},
    'UNDER_2_5': {'min_prob': 0.50, 'min_edge': 5},
}

# No filtro:
threshold = MARKET_THRESHOLDS[market]
if probability < threshold['min_prob'] or edge < threshold['min_edge']:
    return None
```

**Impacto estimado:**
- ‚úÖ Reduzir predictions DRAW de 36.6% para ~25-28%
- ‚úÖ Aumentar accuracy DRAW de 17.3% para ~25-30%
- ‚úÖ Aumentar accuracy geral de 34.3% para ~40-45%

#### Solu√ß√£o 3: Usar ML Classifier para escolher outcome

```python
def _use_ml_classifier(self, match: Match) -> Optional[str]:
    """
    Usa modelo ML treinado para escolher o melhor outcome
    ao inv√©s de gerar todos com edge positivo
    """
    features = self._extract_features(match)

    # Classificador 1X2 (j√° treinado)
    prediction = self.ml_1x2_classifier.predict(features)
    probability = self.ml_1x2_classifier.predict_proba(features)

    # S√≥ retornar se confidence > 40%
    if max(probability) < 0.40:
        return None

    return prediction
```

**Impacto estimado:**
- ‚úÖ Accuracy baseada no modelo treinado (~45.6% atual)
- ‚ö†Ô∏è Requer retreinar com dados balanceados
- ‚è±Ô∏è Mais complexo de implementar

### Recomenda√ß√£o

**Implementar Solu√ß√£o 1 + Solu√ß√£o 2 combinadas:**

1. Para 1X2 (HOME_WIN, DRAW, AWAY_WIN): Selecionar APENAS o melhor outcome
2. Para BTTS e O/U: Usar thresholds ajustados
3. Validar com dados hist√≥ricos antes de rodar em produ√ß√£o

---

## ‚ö†Ô∏è PROBLEMA #2: 63.4% PREDICTIONS N√ÉO VALIDADAS

### Dados do Problema

- Total predictions: 14,783
- Validadas: 5,407 (36.6%)
- Pendentes: 9,376 (63.4%)

### Causas

1. **Jogos futuros:** Predictions geradas para jogos que ainda n√£o aconteceram
2. **Falta de limpeza:** Predictions de jogos antigos cancelados n√£o removidas
3. **Bug anterior:** Antes da corre√ß√£o, muitas predictions ficaram √≥rf√£s

### Solu√ß√µes

#### Solu√ß√£o 1: Limpar predictions de jogos cancelados/adiados

```python
def cleanup_invalid_predictions():
    """
    Remove predictions de jogos que foram cancelados, adiados ou muito antigos
    """
    from datetime import datetime, timedelta

    # Jogos cancelados/adiados
    cancelled_matches = db.query(Match).filter(
        Match.status.in_(['CANC', 'PST', 'ABD', 'AWD', 'WO'])
    ).all()

    for match in cancelled_matches:
        db.query(Prediction).filter(
            Prediction.match_id == match.id
        ).delete()

    # Predictions de jogos antigos (>30 dias) ainda n√£o finalizados
    cutoff = datetime.now() - timedelta(days=30)
    old_predictions = db.query(Prediction).join(Match).filter(
        Match.match_date < cutoff,
        Match.status.in_(['NS', 'TBD', 'SCHEDULED']),
        Prediction.is_validated == False
    ).all()

    count = db.query(Prediction).filter(
        Prediction.id.in_([p.id for p in old_predictions])
    ).delete(synchronize_session=False)

    db.commit()

    print(f"üßπ {count} predictions antigas removidas")
```

#### Solu√ß√£o 2: Validar predictions pendentes em batch

```python
def validate_pending_predictions_batch():
    """
    Valida todas predictions pendentes de jogos j√° finalizados
    """
    pending = db.query(Prediction).join(Match).filter(
        Prediction.is_validated == False,
        Match.status == 'FT',
        Match.home_score.isnot(None)
    ).count()

    if pending > 0:
        run_historical_validation(db)
        print(f"‚úÖ {pending} predictions validadas")
```

#### Solu√ß√£o 3: Adicionar job de limpeza ao scheduler

```python
# Em app/core/scheduler.py

def cleanup_old_predictions_job():
    """
    Job para limpar predictions antigas/inv√°lidas
    Executa diariamente √†s 04:00
    """
    logger.info("üßπ Limpando predictions antigas...")

    db = get_db_session()
    try:
        cleanup_invalid_predictions(db)
        validate_pending_predictions_batch(db)
    finally:
        db.close()

# Adicionar ao scheduler
scheduler.add_job(
    cleanup_old_predictions_job,
    trigger=CronTrigger(hour=4, minute=0),
    id='cleanup_predictions',
    name='üßπ Limpeza de Predictions (di√°rio 04:00)',
    replace_existing=True
)
```

**Impacto estimado:**
- ‚úÖ Aumentar % de validadas de 36.6% para ~80%+
- ‚úÖ Dados mais limpos para ML retraining
- ‚úÖ Menos ru√≠do nos dashboards

---

## ‚ö†Ô∏è PROBLEMA #3: CONFIDENCE SCORES DESCALIBRADOS

### Dados do Problema

- Confidence m√©dio GREENS: 59.3%
- Confidence m√©dio REDS: 45.4%
- Diferen√ßa: 13.8%

**Predictions com confidence >= 70%:**
- Total: 1,968
- REDS: 871 (44.3%) ‚ö†Ô∏è

**Problema:** Mesmo predictions com alta confidence (70%+) t√™m 44.3% de erro!

### Causa Raiz

```python
# app/services/ml_prediction_generator.py (linha 434)
confidence_score = probability  # Usando probabilidade direta!
```

A probabilidade do Poisson N√ÉO √© o mesmo que confidence. Confidence deveria refletir:
1. Hist√≥rico de accuracy do modelo
2. Qualidade dos dados de entrada
3. Incerteza das features

### Solu√ß√µes

#### Solu√ß√£o 1: Calibrar confidence com hist√≥rico

```python
def calibrate_confidence(self, raw_probability: float, market: str) -> float:
    """
    Calibra confidence baseado no hist√≥rico de accuracy do market

    Usa Platt Scaling ou Isotonic Regression
    """
    # Buscar accuracy hist√≥rica do market
    historical_accuracy = self._get_historical_accuracy(market)

    # F√≥rmula simples:
    # confidence = raw_probability * (historical_accuracy / 0.5)
    # Isso ajusta para cima se accuracy > 50%, para baixo se < 50%

    calibration_factor = historical_accuracy / 0.5
    calibrated = raw_probability * calibration_factor

    # Clamp entre 0 e 1
    return max(0.0, min(1.0, calibrated))
```

**Exemplo:**
- DRAW tem 17.3% accuracy (calibration_factor = 0.346)
- Probabilidade Poisson: 30%
- Confidence calibrado: 30% * 0.346 = 10.4%
- Resultado: N√£o gerar prediction (< threshold)

#### Solu√ß√£o 2: Usar sklearn CalibratedClassifierCV

```python
from sklearn.calibration import CalibratedClassifierCV

# Ap√≥s treinar modelo
calibrated_model = CalibratedClassifierCV(
    base_model,
    method='isotonic',  # ou 'sigmoid'
    cv=5
)
calibrated_model.fit(X_train, y_train)

# Usar para predictions
probabilities = calibrated_model.predict_proba(features)
# Estas probabilidades ser√£o calibradas!
```

#### Solu√ß√£o 3: Adicionar penalidade por incerteza

```python
def calculate_confidence_with_uncertainty(self, probability: float, match: Match) -> float:
    """
    Ajusta confidence baseado em fatores de incerteza
    """
    uncertainty_factors = {
        'missing_stats': 0.9 if (not match.home_stats or not match.away_stats) else 1.0,
        'no_odds': 0.85 if not match.odds else 1.0,
        'low_sample_h2h': 0.95 if match.h2h_count < 3 else 1.0,
        'new_team': 0.9 if match.team_is_new else 1.0,
    }

    # Multiplicar todos os fatores
    total_uncertainty = 1.0
    for factor in uncertainty_factors.values():
        total_uncertainty *= factor

    return probability * total_uncertainty
```

### Recomenda√ß√£o

**Implementar todas as 3 solu√ß√µes:**
1. Calibrar com hist√≥rico (r√°pido, impacto imediato)
2. sklearn CalibratedClassifierCV no pr√≥ximo retraining
3. Penalidade por incerteza (melhora qualidade)

**Impacto estimado:**
- ‚úÖ Confidence scores refletem accuracy real
- ‚úÖ Menos predictions com alta confidence que d√£o RED
- ‚úÖ Melhor para AI Agent usar como input

---

## üìã PLANO DE IMPLEMENTA√á√ÉO

### Fase 1: Corre√ß√µes Cr√≠ticas (2-3 dias)

**Prioridade P0:**

1. **Selecionar melhor outcome 1X2**
   - [ ] Implementar `_select_best_1x2_outcome()`
   - [ ] Ajustar `generate_daily_predictions()` para usar
   - [ ] Testar com dados hist√≥ricos
   - [ ] Validar melhoria de accuracy

2. **Ajustar thresholds por market**
   - [ ] Criar `MARKET_THRESHOLDS` dict
   - [ ] DRAW: min_prob=0.35, min_edge=15
   - [ ] Aplicar nos filtros
   - [ ] Testar gera√ß√£o de predictions

3. **Limpar predictions pendentes**
   - [ ] Implementar `cleanup_invalid_predictions()`
   - [ ] Rodar manualmente
   - [ ] Validar que removed correto
   - [ ] Adicionar ao scheduler

**Tempo estimado:** 2-3 dias
**Impacto esperado:** Accuracy 34.3% ‚Üí 45-50%

### Fase 2: Calibra√ß√£o e Otimiza√ß√£o (3-5 dias)

**Prioridade P1:**

1. **Calibrar confidence scores**
   - [ ] Implementar `calibrate_confidence()`
   - [ ] Calcular accuracy hist√≥rica por market
   - [ ] Aplicar calibra√ß√£o
   - [ ] Validar que confidence = accuracy real

2. **Retreinar ML com dados balanceados**
   - [ ] Preparar dataset balanceado (SMOTE ou undersampling)
   - [ ] Retreinar 1x2_classifier
   - [ ] Retreinar btts_classifier
   - [ ] Validar melhorias

3. **Adicionar valida√ß√£o cont√≠nua**
   - [ ] Job de valida√ß√£o a cada 1h
   - [ ] Job de limpeza di√°rio
   - [ ] Logs detalhados
   - [ ] M√©tricas de accuracy por market

**Tempo estimado:** 3-5 dias
**Impacto esperado:** Accuracy 45-50% ‚Üí 55-60%

### Fase 3: Refinamento (5-7 dias)

**Prioridade P2:**

1. **Features avan√ßadas**
   - [ ] Adicionar form (√∫ltimos 5 jogos)
   - [ ] Adicionar H2H history
   - [ ] Adicionar injuries impact
   - [ ] Adicionar weather conditions

2. **Multi-model ensemble**
   - [ ] Testar XGBoost, LightGBM
   - [ ] Implementar voting classifier
   - [ ] Comparar accuracy
   - [ ] Escolher melhor combina√ß√£o

3. **AI Agent otimiza√ß√£o**
   - [ ] Usar confidence calibrado
   - [ ] Refinar prompts
   - [ ] Validar adjustments
   - [ ] Medir impacto real

**Tempo estimado:** 5-7 dias
**Impacto esperado:** Accuracy 55-60% ‚Üí 60-65%

---

## üìä M√âTRICAS DE SUCESSO

### Targets por Fase

| Fase | Accuracy Target | DRAW Accuracy | % Validadas | Confidence Calibration |
|------|----------------|---------------|-------------|------------------------|
| **Atual** | 34.3% | 17.3% | 36.6% | ‚ùå Descalibrado |
| **Fase 1** | 45-50% | 30-35% | 80%+ | ‚ö†Ô∏è Parcial |
| **Fase 2** | 55-60% | 35-40% | 90%+ | ‚úÖ Calibrado |
| **Fase 3** | 60-65% | 40-45% | 95%+ | ‚úÖ Otimizado |

### KPIs a Monitorar

**Diariamente:**
- Accuracy geral
- Accuracy por market
- % predictions validadas
- Confidence m√©dio GREEN vs RED

**Semanalmente:**
- Melhoria de accuracy ML ap√≥s retraining
- ROI simulado de predictions
- Distribui√ß√£o de outcomes gerados

**Mensalmente:**
- Accuracy trend (deve subir)
- Predictions/dia geradas
- GREEN/RED ratio por league

---

## üöÄ PR√ìXIMOS PASSOS IMEDIATOS

### Hoje (20/10/2025)

1. ‚úÖ An√°lise completa - CONCLU√çDO
2. ‚úÖ Identificar problemas - CONCLU√çDO
3. ‚úÖ Criar plano de corre√ß√£o - CONCLU√çDO
4. ‚è≥ **PR√ìXIMO:** Implementar Fase 1 - Corre√ß√µes Cr√≠ticas

### Amanh√£ (21/10/2025)

1. Implementar `_select_best_1x2_outcome()`
2. Ajustar thresholds DRAW
3. Testar nova gera√ß√£o de predictions
4. Validar melhoria de accuracy

### Pr√≥xima Semana

1. Completar Fase 1
2. Iniciar Fase 2 (calibra√ß√£o)
3. Monitorar m√©tricas di√°rias
4. Ajustar conforme necess√°rio

---

## ‚ö†Ô∏è RISCOS E MITIGA√á√ïES

| Risco | Probabilidade | Impacto | Mitiga√ß√£o |
|-------|---------------|---------|-----------|
| Accuracy n√£o melhorar ap√≥s Fase 1 | BAIXA | ALTO | Rollback + investigar features |
| Over-fitting ao corrigir DRAW | M√âDIA | M√âDIO | Validar com dados out-of-sample |
| Muito poucas predictions geradas | M√âDIA | M√âDIO | Ajustar thresholds gradualmente |
| ML retraining piorar modelos | BAIXA | ALTO | Backup de modelos, A/B test |

---

## üìù CONCLUS√ÉO

**Estado Atual:**
- ‚ùå Accuracy 34.3% (abaixo da meta de 60%)
- ‚ùå ML prevendo DRAW demais (36.6% vs 22.5% real)
- ‚ùå DRAW com apenas 17.3% accuracy
- ‚ùå 63.4% predictions n√£o validadas
- ‚ùå Confidence scores descalibrados

**Ap√≥s Implementa√ß√£o Completa:**
- ‚úÖ Accuracy 60-65% (meta atingida)
- ‚úÖ Distribui√ß√£o de outcomes balanceada
- ‚úÖ DRAW com 40-45% accuracy
- ‚úÖ 95%+ predictions validadas
- ‚úÖ Confidence scores calibrados = accuracy real

**Tempo Total Estimado:** 10-15 dias
**Prioridade:** üî¥ CR√çTICA

**Antes de pensar em CI/CD e cloud deployment, precisamos ter uma pipeline de ML funcionando corretamente com 60%+ accuracy!**

---

**Criado por:** Equipe de desenvolvimento
**Data:** 2025-10-20
**Vers√£o:** 1.0
