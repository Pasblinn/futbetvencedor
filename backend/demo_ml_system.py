#!/usr/bin/env python3
"""
ðŸŽ¯ DEMONSTRAÃ‡ÃƒO DO SISTEMA ML COMPLETO
Script para demonstrar todas as funcionalidades do sistema de logging e feedback ML

Funcionalidades demonstradas:
1. Sistema de logging de prediÃ§Ãµes
2. AnÃ¡lise de performance
3. Feedback automÃ¡tico para ML
4. Endpoints de monitoramento
5. ExportaÃ§Ã£o de dados de aprendizado
"""

import asyncio
import sys
import json
from pathlib import Path
from datetime import datetime, timedelta
import logging
from sqlalchemy.orm import Session

sys.path.insert(0, str(Path(__file__).parent))

from app.core.database import get_db
from app.models import PredictionLog, ModelPerformance, Match, Prediction
from app.services.prediction_logger import PredictionLogger

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MLSystemDemo:
    """DemonstraÃ§Ã£o completa do sistema ML"""
    
    def __init__(self):
        pass
    
    async def run_complete_demo(self):
        """Executa demonstraÃ§Ã£o completa do sistema"""
        print("ðŸŽ¯ DEMONSTRAÃ‡ÃƒO DO SISTEMA ML COMPLETO")
        print("=" * 60)
        
        db = next(get_db())
        
        try:
            # 1. Mostrar estatÃ­sticas atuais
            print("\n1ï¸âƒ£ ESTATÃSTICAS ATUAIS DO SISTEMA")
            await self.show_system_stats(db)
            
            # 2. Demonstrar sistema de logging
            print("\n2ï¸âƒ£ SISTEMA DE LOGGING DE PREDIÃ‡Ã•ES")
            await self.demonstrate_logging_system(db)
            
            # 3. Mostrar anÃ¡lise de performance
            print("\n3ï¸âƒ£ ANÃLISE DE PERFORMANCE")
            await self.demonstrate_performance_analysis(db)
            
            # 4. Demonstrar feedback para ML
            print("\n4ï¸âƒ£ SISTEMA DE FEEDBACK PARA ML")
            await self.demonstrate_ml_feedback(db)
            
            # 5. Mostrar endpoints disponÃ­veis
            print("\n5ï¸âƒ£ ENDPOINTS DISPONÃVEIS")
            await self.show_available_endpoints()
            
            # 6. Demonstrar exportaÃ§Ã£o de dados
            print("\n6ï¸âƒ£ EXPORTAÃ‡ÃƒO DE DADOS DE APRENDIZADO")
            await self.demonstrate_data_export(db)
            
            print("\nðŸŽ‰ DEMONSTRAÃ‡ÃƒO COMPLETA FINALIZADA!")
            print("=" * 60)
            
        except Exception as e:
            logger.error(f"âŒ Erro na demonstraÃ§Ã£o: {e}")
        finally:
            db.close()
    
    async def show_system_stats(self, db: Session):
        """Mostra estatÃ­sticas atuais do sistema"""
        try:
            # Contar prediÃ§Ãµes
            total_predictions = db.query(Prediction).count()
            
            # Contar logs
            total_logs = db.query(PredictionLog).count()
            analyzed_logs = db.query(PredictionLog).filter(
                PredictionLog.analyzed_at.isnot(None)
            ).count()
            
            # Contar jogos
            total_matches = db.query(Match).count()
            finished_matches = db.query(Match).filter(
                Match.status == 'finished'
            ).count()
            
            print(f"   ðŸ“Š PrediÃ§Ãµes totais: {total_predictions}")
            print(f"   ðŸ“ Logs de prediÃ§Ãµes: {total_logs}")
            print(f"   âœ… Logs analisados: {analyzed_logs}")
            print(f"   âš½ Jogos totais: {total_matches}")
            print(f"   ðŸ Jogos finalizados: {finished_matches}")
            
            # Calcular integraÃ§Ã£o
            integration_rate = (total_logs / total_predictions * 100) if total_predictions > 0 else 0
            print(f"   ðŸ”— Taxa de integraÃ§Ã£o: {integration_rate:.1f}%")
            
        except Exception as e:
            print(f"   âŒ Erro ao obter estatÃ­sticas: {e}")
    
    async def demonstrate_logging_system(self, db: Session):
        """Demonstra o sistema de logging"""
        try:
            logger_service = PredictionLogger(db)
            
            # Mostrar logs recentes
            recent_logs = db.query(PredictionLog).order_by(
                PredictionLog.created_at.desc()
            ).limit(3).all()
            
            print(f"   ðŸ“‹ Ãšltimos {len(recent_logs)} logs de prediÃ§Ãµes:")
            
            for log in recent_logs:
                status = "âœ… Analisado" if log.analyzed_at else "â³ Pendente"
                print(f"      - Match {log.match_id}: {log.predicted_outcome} "
                      f"(ConfianÃ§a: {log.confidence_score:.2%}) - {status}")
            
            # Mostrar distribuiÃ§Ã£o por modelo
            models = db.query(PredictionLog.model_name, 
                            db.func.count(PredictionLog.id)).group_by(
                PredictionLog.model_name
            ).all()
            
            print(f"   ðŸ¤– DistribuiÃ§Ã£o por modelo:")
            for model, count in models:
                print(f"      - {model}: {count} prediÃ§Ãµes")
            
        except Exception as e:
            print(f"   âŒ Erro na demonstraÃ§Ã£o de logging: {e}")
    
    async def demonstrate_performance_analysis(self, db: Session):
        """Demonstra anÃ¡lise de performance"""
        try:
            # Buscar logs analisados
            analyzed_logs = db.query(PredictionLog).filter(
                PredictionLog.analyzed_at.isnot(None)
            ).all()
            
            if analyzed_logs:
                # Calcular mÃ©tricas
                total = len(analyzed_logs)
                correct = len([log for log in analyzed_logs if log.was_correct])
                accuracy = correct / total if total > 0 else 0
                
                avg_confidence = sum(log.confidence_score for log in analyzed_logs) / total
                avg_feedback = sum(log.feedback_score for log in analyzed_logs) / total
                
                print(f"   ðŸ“Š MÃ©tricas de Performance:")
                print(f"      - AcurÃ¡cia: {accuracy:.2%}")
                print(f"      - ConfianÃ§a mÃ©dia: {avg_confidence:.2%}")
                print(f"      - Feedback mÃ©dio: {avg_feedback:.2%}")
                
                # Performance por liga
                leagues = {}
                for log in analyzed_logs:
                    league = log.league
                    if league not in leagues:
                        leagues[league] = {"total": 0, "correct": 0}
                    leagues[league]["total"] += 1
                    if log.was_correct:
                        leagues[league]["correct"] += 1
                
                print(f"   ðŸ† Performance por liga:")
                for league, stats in leagues.items():
                    league_accuracy = stats["correct"] / stats["total"] if stats["total"] > 0 else 0
                    print(f"      - {league}: {league_accuracy:.2%} ({stats['correct']}/{stats['total']})")
            else:
                print(f"   âš ï¸ Nenhuma prediÃ§Ã£o analisada ainda")
                print(f"   ðŸ’¡ Execute o script de anÃ¡lise para processar jogos finalizados")
            
        except Exception as e:
            print(f"   âŒ Erro na anÃ¡lise de performance: {e}")
    
    async def demonstrate_ml_feedback(self, db: Session):
        """Demonstra sistema de feedback para ML"""
        try:
            # Buscar logs com feedback
            feedback_logs = db.query(PredictionLog).filter(
                PredictionLog.feedback_score.isnot(None)
            ).limit(5).all()
            
            if feedback_logs:
                print(f"   ðŸ§  Exemplos de Feedback para ML:")
                
                for log in feedback_logs:
                    feedback_type = "âœ… Positivo" if log.feedback_score > 0.5 else "âŒ Negativo"
                    print(f"      - Match {log.match_id}: {feedback_type} "
                          f"(Score: {log.feedback_score:.2f})")
                    
                    if log.learning_insights:
                        insights = log.learning_insights
                        if isinstance(insights, dict) and 'key_learning' in insights:
                            for learning in insights['key_learning'][:1]:  # Mostrar apenas 1
                                print(f"        ðŸ’¡ {learning}")
                
                # Mostrar insights agregados
                high_feedback = len([log for log in feedback_logs if log.feedback_score > 0.7])
                low_feedback = len([log for log in feedback_logs if log.feedback_score < 0.3])
                
                print(f"   ðŸ“ˆ DistribuiÃ§Ã£o de Feedback:")
                print(f"      - Alto feedback (>0.7): {high_feedback}")
                print(f"      - Baixo feedback (<0.3): {low_feedback}")
            else:
                print(f"   âš ï¸ Nenhum feedback gerado ainda")
                print(f"   ðŸ’¡ Feedback Ã© gerado automaticamente apÃ³s anÃ¡lise de jogos")
            
        except Exception as e:
            print(f"   âŒ Erro na demonstraÃ§Ã£o de feedback: {e}")
    
    async def show_available_endpoints(self):
        """Mostra endpoints disponÃ­veis"""
        endpoints = [
            ("GET", "/api/v1/ml/performance/overview", "Overview de performance do ML"),
            ("GET", "/api/v1/ml/performance/detailed", "Performance detalhada por modelo"),
            ("GET", "/api/v1/ml/learning/insights", "Insights de aprendizado"),
            ("GET", "/api/v1/ml/predictions/logs", "Logs de prediÃ§Ãµes"),
            ("POST", "/api/v1/ml/analyze/finished-matches", "ForÃ§ar anÃ¡lise de jogos finalizados")
        ]
        
        print(f"   ðŸŒ Endpoints de ML Performance:")
        for method, endpoint, description in endpoints:
            print(f"      {method} {endpoint}")
            print(f"         â†’ {description}")
    
    async def demonstrate_data_export(self, db: Session):
        """Demonstra exportaÃ§Ã£o de dados"""
        try:
            # Buscar dados para exportaÃ§Ã£o
            logs_to_export = db.query(PredictionLog).filter(
                PredictionLog.analyzed_at.isnot(None)
            ).limit(10).all()
            
            if logs_to_export:
                export_data = {
                    "export_timestamp": datetime.now().isoformat(),
                    "total_predictions": len(logs_to_export),
                    "predictions": []
                }
                
                for log in logs_to_export:
                    prediction_data = {
                        "id": log.id,
                        "match_id": log.match_id,
                        "predicted_outcome": log.predicted_outcome,
                        "actual_outcome": log.actual_outcome,
                        "confidence_score": log.confidence_score,
                        "was_correct": log.was_correct,
                        "feedback_score": log.feedback_score,
                        "league": log.league,
                        "model_name": log.model_name,
                        "created_at": log.created_at.isoformat()
                    }
                    export_data["predictions"].append(prediction_data)
                
                # Salvar arquivo de demonstraÃ§Ã£o
                demo_file = "ml_demo_export.json"
                with open(demo_file, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, indent=2, ensure_ascii=False)
                
                print(f"   ðŸ“¤ Dados exportados para: {demo_file}")
                print(f"   ðŸ“Š {len(logs_to_export)} prediÃ§Ãµes exportadas")
                print(f"   ðŸ’¾ Arquivo pronto para anÃ¡lise externa ou retreinamento")
            else:
                print(f"   âš ï¸ Nenhum dado analisado para exportaÃ§Ã£o")
                print(f"   ðŸ’¡ Execute anÃ¡lise de jogos finalizados primeiro")
            
        except Exception as e:
            print(f"   âŒ Erro na exportaÃ§Ã£o: {e}")
    
    def show_usage_instructions(self):
        """Mostra instruÃ§Ãµes de uso"""
        print("\nðŸ“š INSTRUÃ‡Ã•ES DE USO DO SISTEMA:")
        print("=" * 60)
        print("1. ðŸ”„ AnÃ¡lise AutomÃ¡tica:")
        print("   python analyze_predictions.py")
        print()
        print("2. ðŸ“Š Monitoramento via API:")
        print("   curl http://localhost:8000/api/v1/ml/performance/overview")
        print()
        print("3. ðŸ“ Logs de PrediÃ§Ãµes:")
        print("   curl http://localhost:8000/api/v1/ml/predictions/logs")
        print()
        print("4. ðŸ§  Insights de Aprendizado:")
        print("   curl http://localhost:8000/api/v1/ml/learning/insights")
        print()
        print("5. ðŸ”„ ForÃ§ar AnÃ¡lise:")
        print("   curl -X POST http://localhost:8000/api/v1/ml/analyze/finished-matches")

async def main():
    """FunÃ§Ã£o principal"""
    demo = MLSystemDemo()
    
    # Executar demonstraÃ§Ã£o
    await demo.run_complete_demo()
    
    # Mostrar instruÃ§Ãµes
    demo.show_usage_instructions()

if __name__ == "__main__":
    asyncio.run(main())
