"""
ğŸ•·ï¸ ADVANCED TABLE SCRAPER - Scraper robusto para contornar bloqueios
Implementa mÃºltiplas estratÃ©gias para extrair tabelas de sites de futebol
Contorna 403, 401, 404, 500 com fallbacks inteligentes
"""

import requests
import pandas as pd
import time
import random
import os
import re
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin, urlparse
import warnings
warnings.filterwarnings('ignore')

# Tentar importar bibliotecas opcionais
try:
    from requests_html import HTMLSession
    HAS_REQUESTS_HTML = True
except ImportError:
    HAS_REQUESTS_HTML = False
    print("âš ï¸ requests-html nÃ£o instalado. Use: pip install requests-html")

try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.chrome.service import Service
    HAS_SELENIUM = True
except ImportError:
    HAS_SELENIUM = False
    print("âš ï¸ selenium nÃ£o instalado. Use: pip install selenium webdriver-manager")

class AdvancedTableScraper:
    """
    ğŸ•·ï¸ Scraper avanÃ§ado com mÃºltiplas estratÃ©gias anti-bloqueio
    """

    def __init__(self, use_proxies: bool = False):
        """
        Inicializar scraper com configuraÃ§Ãµes

        Args:
            use_proxies: Se deve usar proxies (implementaÃ§Ã£o futura)
        """
        self.use_proxies = use_proxies
        self.session = requests.Session()

        # Lista de User-Agents populares para rotaÃ§Ã£o
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:90.0) Gecko/20100101 Firefox/90.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:90.0) Gecko/20100101 Firefox/90.0'
        ]

        # Headers padrÃ£o que simulam navegador real
        self.base_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8,en-US;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
            'sec-ch-ua': '"Chromium";v="91", " Not;A Brand";v="99"',
            'sec-ch-ua-mobile': '?0',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1'
        }

        # Configurar diretÃ³rio de saÃ­da
        self.output_dir = "scraped_tables"
        os.makedirs(self.output_dir, exist_ok=True)

    def get_random_headers(self, referer: Optional[str] = None) -> Dict[str, str]:
        """
        Gerar headers aleatÃ³rios para simular navegador real

        Args:
            referer: URL de referÃªncia opcional

        Returns:
            DicionÃ¡rio com headers
        """
        headers = self.base_headers.copy()
        headers['User-Agent'] = random.choice(self.user_agents)

        if referer:
            headers['Referer'] = referer

        return headers

    def fetch_with_requests(self, url: str, retries: int = 3) -> Optional[str]:
        """
        EstratÃ©gia 1: Requests simples com headers e retry

        Args:
            url: URL alvo
            retries: NÃºmero de tentativas

        Returns:
            HTML da pÃ¡gina ou None se falhar
        """
        print(f"ğŸŒ Tentativa 1: requests simples para {url}")

        for attempt in range(retries):
            try:
                headers = self.get_random_headers()

                # Delay aleatÃ³rio entre tentativas
                if attempt > 0:
                    delay = random.uniform(2, 5)
                    print(f"â³ Aguardando {delay:.1f}s antes da tentativa {attempt + 1}")
                    time.sleep(delay)

                response = self.session.get(
                    url,
                    headers=headers,
                    timeout=30,
                    allow_redirects=True
                )

                print(f"ğŸ“¡ Status Code: {response.status_code}")

                if response.status_code == 200:
                    print("âœ… Sucesso com requests simples!")
                    return response.text

                elif response.status_code in [403, 401]:
                    print(f"ğŸš« Bloqueado ({response.status_code}), tentando novamente...")
                    continue

                elif response.status_code in [404]:
                    print("âŒ PÃ¡gina nÃ£o encontrada (404)")
                    return None

                elif response.status_code in [500, 502, 503]:
                    print(f"ğŸ”¥ Erro do servidor ({response.status_code}), tentando novamente...")
                    continue

                else:
                    print(f"âš ï¸ Status inesperado: {response.status_code}")
                    continue

            except requests.exceptions.RequestException as e:
                print(f"âŒ Erro na requisiÃ§Ã£o: {e}")
                continue

        print("âŒ Falha em todas as tentativas com requests")
        return None

    def fetch_with_requests_html(self, url: str) -> Optional[str]:
        """
        EstratÃ©gia 2: requests-html com renderizaÃ§Ã£o JavaScript

        Args:
            url: URL alvo

        Returns:
            HTML renderizado ou None se falhar
        """
        if not HAS_REQUESTS_HTML:
            print("âš ï¸ requests-html nÃ£o disponÃ­vel, pulando...")
            return None

        print(f"ğŸš€ Tentativa 2: requests-html com JS para {url}")

        try:
            session = HTMLSession()

            headers = self.get_random_headers()
            for key, value in headers.items():
                session.headers[key] = value

            r = session.get(url, timeout=30)
            print(f"ğŸ“¡ Status Code: {r.status_code}")

            if r.status_code == 200:
                # Renderizar JavaScript
                print("ğŸ”„ Renderizando JavaScript...")
                r.html.render(timeout=20, wait=2)
                print("âœ… Sucesso com requests-html!")
                return r.html.html

        except Exception as e:
            print(f"âŒ Erro com requests-html: {e}")

        return None

    def fetch_with_selenium(self, url: str) -> Optional[str]:
        """
        EstratÃ©gia 3: Selenium com Chrome headless (Ãºltimo recurso)

        Args:
            url: URL alvo

        Returns:
            HTML da pÃ¡gina ou None se falhar
        """
        if not HAS_SELENIUM:
            print("âš ï¸ selenium nÃ£o disponÃ­vel, pulando...")
            return None

        print(f"ğŸ¤– Tentativa 3: Selenium headless para {url}")

        driver = None
        try:
            # Configurar Chrome headless
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument(f'--user-agent={random.choice(self.user_agents)}')

            # Usar webdriver-manager para gerenciar ChromeDriver automaticamente
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
            driver.get(url)

            # Aguardar carregamento
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            # Aguardar possÃ­veis tabelas carregarem
            time.sleep(3)

            html = driver.page_source
            print("âœ… Sucesso com Selenium!")
            return html

        except Exception as e:
            print(f"âŒ Erro com Selenium: {e}")
            return None

        finally:
            if driver:
                driver.quit()

    def extract_tables_from_html(self, html: str, url: str) -> List[Tuple[pd.DataFrame, str]]:
        """
        Extrair todas as tabelas do HTML usando pandas

        Args:
            html: HTML da pÃ¡gina
            url: URL original (para contexto)

        Returns:
            Lista de tuplas (DataFrame, nome_da_tabela)
        """
        print("ğŸ“Š Extraindo tabelas com pandas...")

        tables_found = []

        try:
            # Tentar extrair tabelas com pandas
            tables = pd.read_html(html, header=0)
            print(f"ğŸ¯ Encontradas {len(tables)} tabelas")

            for i, table in enumerate(tables):
                # Gerar nome da tabela
                table_name = f"table_{i+1}"

                # Tentar encontrar tÃ­tulo da tabela no HTML
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(html, 'html.parser')

                    # Procurar por tÃ­tulos prÃ³ximos Ã s tabelas
                    table_tags = soup.find_all('table')
                    if i < len(table_tags):
                        table_tag = table_tags[i]

                        # Buscar tÃ­tulo em elementos anteriores
                        for sibling in table_tag.find_all_previous(['h1', 'h2', 'h3', 'h4', 'caption']):
                            if sibling.get_text().strip():
                                title = sibling.get_text().strip()
                                # Limpar tÃ­tulo para nome de arquivo
                                title = re.sub(r'[^\w\s-]', '', title)
                                title = re.sub(r'[-\s]+', '_', title)
                                table_name = title[:50] if title else table_name
                                break

                except ImportError:
                    print("âš ï¸ BeautifulSoup nÃ£o disponÃ­vel para melhor detecÃ§Ã£o de tÃ­tulos")
                except Exception as e:
                    print(f"âš ï¸ Erro ao extrair tÃ­tulo da tabela: {e}")

                tables_found.append((table, table_name))

        except ValueError as e:
            print(f"âŒ Nenhuma tabela encontrada: {e}")
        except Exception as e:
            print(f"âŒ Erro ao extrair tabelas: {e}")

        return tables_found

    def clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Limpar DataFrame removendo multi-Ã­ndices e espaÃ§os

        Args:
            df: DataFrame original

        Returns:
            DataFrame limpo
        """
        # Copiar DataFrame
        cleaned_df = df.copy()

        # Remover multi-Ã­ndices das colunas
        if isinstance(cleaned_df.columns, pd.MultiIndex):
            # Concatenar nÃ­veis do multi-Ã­ndice
            cleaned_df.columns = [
                '_'.join(str(col).strip() for col in column if str(col) != 'nan')
                for column in cleaned_df.columns
            ]

        # Limpar nomes das colunas
        cleaned_df.columns = [
            re.sub(r'\s+', '_', str(col).strip().replace(' ', '_'))
            for col in cleaned_df.columns
        ]

        # Remover colunas completamente vazias
        cleaned_df = cleaned_df.dropna(axis=1, how='all')

        # Remover linhas completamente vazias
        cleaned_df = cleaned_df.dropna(axis=0, how='all')

        return cleaned_df

    def save_tables_to_csv(self, tables: List[Tuple[pd.DataFrame, str]], url: str) -> List[str]:
        """
        Salvar tabelas em arquivos CSV

        Args:
            tables: Lista de tuplas (DataFrame, nome)
            url: URL original

        Returns:
            Lista de arquivos salvos
        """
        saved_files = []

        # Gerar prefixo baseado na URL
        domain = urlparse(url).netloc.replace('www.', '').replace('.', '_')
        timestamp = int(time.time())

        for i, (table, table_name) in enumerate(tables):
            try:
                # Limpar DataFrame
                cleaned_table = self.clean_dataframe(table)

                if cleaned_table.empty:
                    print(f"âš ï¸ Tabela {i+1} estÃ¡ vazia apÃ³s limpeza, pulando...")
                    continue

                # Gerar nome do arquivo
                filename = f"{domain}_{table_name}_{timestamp}.csv"
                filepath = os.path.join(self.output_dir, filename)

                # Salvar CSV
                cleaned_table.to_csv(filepath, index=False, encoding='utf-8')
                print(f"ğŸ’¾ Salvo: {filename} ({len(cleaned_table)} linhas, {len(cleaned_table.columns)} colunas)")

                saved_files.append(filepath)

            except Exception as e:
                print(f"âŒ Erro salvando tabela {i+1}: {e}")
                continue

        return saved_files

    def scrape_url(self, url: str) -> Dict:
        """
        MÃ©todo principal para fazer scraping de uma URL

        Args:
            url: URL alvo

        Returns:
            DicionÃ¡rio com resultados do scraping
        """
        print(f"ğŸ•·ï¸ INICIANDO SCRAPING AVANÃ‡ADO")
        print(f"ğŸ¯ URL: {url}")
        print("=" * 70)

        result = {
            'url': url,
            'success': False,
            'method_used': None,
            'tables_found': 0,
            'files_saved': [],
            'error': None
        }

        html = None

        # EstratÃ©gia 1: Requests simples
        html = self.fetch_with_requests(url)
        if html:
            result['method_used'] = 'requests'

        # EstratÃ©gia 2: requests-html se a primeira falhar
        if not html:
            html = self.fetch_with_requests_html(url)
            if html:
                result['method_used'] = 'requests_html'

        # EstratÃ©gia 3: Selenium se as outras falharem
        if not html:
            html = self.fetch_with_selenium(url)
            if html:
                result['method_used'] = 'selenium'

        # Se conseguimos HTML, extrair tabelas
        if html:
            try:
                tables = self.extract_tables_from_html(html, url)
                result['tables_found'] = len(tables)

                if tables:
                    saved_files = self.save_tables_to_csv(tables, url)
                    result['files_saved'] = saved_files
                    result['success'] = True

                    print(f"\nğŸ‰ SUCESSO!")
                    print(f"ğŸ“Š Tabelas encontradas: {len(tables)}")
                    print(f"ğŸ’¾ Arquivos salvos: {len(saved_files)}")
                    for file in saved_files:
                        print(f"  ğŸ“ {os.path.basename(file)}")

                else:
                    result['error'] = "Nenhuma tabela encontrada na pÃ¡gina"
                    print("âŒ Nenhuma tabela encontrada na pÃ¡gina")

            except Exception as e:
                result['error'] = f"Erro ao processar HTML: {e}"
                print(f"âŒ Erro ao processar HTML: {e}")

        else:
            result['error'] = "Falha em todas as estratÃ©gias de scraping"
            print("âŒ FALHA: Todas as estratÃ©gias falharam")

        return result

def main():
    """
    FunÃ§Ã£o principal interativa
    """
    print("ğŸ•·ï¸ ADVANCED TABLE SCRAPER")
    print("=" * 50)
    print("Scraper robusto que contorna bloqueios 403, 401, 404, 500")
    print("EstratÃ©gias: requests â†’ requests-html â†’ selenium")
    print("=" * 50)

    # Solicitar URL do usuÃ¡rio
    url = input("\nğŸŒ Digite a URL para fazer scraping: ").strip()

    if not url:
        print("âŒ URL nÃ£o fornecida!")
        return

    # Adicionar http:// se necessÃ¡rio
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url

    # Perguntar sobre proxies (futura implementaÃ§Ã£o)
    use_proxies = input("\nğŸ”„ Usar proxies? (y/n) [n]: ").strip().lower() == 'y'

    # Inicializar scraper
    scraper = AdvancedTableScraper(use_proxies=use_proxies)

    # Executar scraping
    print(f"\nğŸš€ Iniciando scraping...")
    result = scraper.scrape_url(url)

    # Mostrar resultado final
    print("\n" + "=" * 70)
    print("ğŸ“‹ RESULTADO FINAL:")
    print(f"âœ… Sucesso: {result['success']}")
    print(f"ğŸ› ï¸ MÃ©todo usado: {result['method_used']}")
    print(f"ğŸ“Š Tabelas encontradas: {result['tables_found']}")
    print(f"ğŸ’¾ Arquivos salvos: {len(result['files_saved'])}")

    if result['error']:
        print(f"âŒ Erro: {result['error']}")

    print(f"ğŸ“ Arquivos salvos em: {scraper.output_dir}/")

if __name__ == "__main__":
    main()