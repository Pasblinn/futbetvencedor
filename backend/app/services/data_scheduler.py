"""
‚è∞ DATA SCHEDULER - AUTOMA√á√ÉO DE COLETA
Scheduler inteligente para automa√ß√£o completa do pipeline de dados

Rotinas autom√°ticas:
- Coleta di√°ria incremental (6h da manh√£)
- Atualiza√ß√£o de jogos ao vivo (a cada 10min durante jogos)
- Sincroniza√ß√£o cache ‚Üí DB (a cada hora)
- Limpeza de cache antigo (semanal)
- Retreinamento de modelos ML (conforme novos dados)
"""

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger
from sqlalchemy.orm import Session
from datetime import datetime, timedelta
import logging
import asyncio

from app.services.data_pipeline import DataPipeline
from app.services.api_quota_manager import APIQuotaManager
from app.services.ml_prediction_generator import run_daily_ml_prediction_generation
from app.services.ai_agent_batch import analyze_unanalyzed_predictions
from app.services.automated_ml_retraining import AutomatedMLRetraining

logger = logging.getLogger(__name__)

class DataScheduler:
    """Scheduler automatizado de coleta de dados"""

    def __init__(self, db: Session):
        self.db = db
        self.pipeline = DataPipeline(db)
        self.quota_manager = APIQuotaManager(db)
        self.scheduler = AsyncIOScheduler()

        # Estado
        self.is_running = False

    def start(self):
        """Iniciar scheduler"""
        if self.is_running:
            logger.warning("‚ö†Ô∏è Scheduler j√° est√° rodando")
            return

        logger.info("üöÄ Iniciando Data Scheduler...")

        # Rotina 1: COLETA DI√ÅRIA INCREMENTAL
        # Todo dia √†s 6h da manh√£ (hor√°rio ideal - poucos jogos acontecendo)
        self.scheduler.add_job(
            self.daily_incremental_job,
            CronTrigger(hour=6, minute=0),
            id='daily_incremental',
            name='Coleta Di√°ria Incremental',
            replace_existing=True
        )
        logger.info("‚úÖ Job agendado: Coleta di√°ria √†s 6h")

        # Rotina 2: ATUALIZA√á√ÉO DE JOGOS AO VIVO
        # A cada 10 minutos durante hor√°rios de jogos
        self.scheduler.add_job(
            self.live_matches_update_job,
            IntervalTrigger(minutes=10),
            id='live_updates',
            name='Atualiza√ß√£o de Jogos ao Vivo',
            replace_existing=True
        )
        logger.info("‚úÖ Job agendado: Atualiza√ß√£o ao vivo a cada 10min")

        # Rotina 3: SINCRONIZA√á√ÉO CACHE ‚Üí DATABASE
        # A cada hora, sincronizar dados do cache para o banco principal
        self.scheduler.add_job(
            self.sync_cache_job,
            IntervalTrigger(hours=1),
            id='sync_cache',
            name='Sincroniza√ß√£o de Cache',
            replace_existing=True
        )
        logger.info("‚úÖ Job agendado: Sync cache a cada 1h")

        # Rotina 4: VERIFICA√á√ÉO DE QUOTA
        # A cada 30 minutos, verificar sa√∫de da quota
        self.scheduler.add_job(
            self.quota_health_check_job,
            IntervalTrigger(minutes=30),
            id='quota_check',
            name='Verifica√ß√£o de Quota',
            replace_existing=True
        )
        logger.info("‚úÖ Job agendado: Verifica√ß√£o de quota a cada 30min")

        # Rotina 5: LIMPEZA DE CACHE
        # Todo domingo √†s 3h da manh√£
        self.scheduler.add_job(
            self.cleanup_cache_job,
            CronTrigger(day_of_week='sun', hour=3, minute=0),
            id='cleanup_cache',
            name='Limpeza de Cache',
            replace_existing=True
        )
        logger.info("‚úÖ Job agendado: Limpeza de cache aos domingos 3h")

        # Rotina 6: GERA√á√ÉO DE ML PREDICTIONS
        # Todo dia √†s 8h da manh√£ (ap√≥s coleta de dados √†s 6h)
        self.scheduler.add_job(
            self.ml_predictions_job,
            CronTrigger(hour=8, minute=0),
            id='ml_predictions',
            name='Gera√ß√£o de ML Predictions (4500/dia)',
            replace_existing=True
        )
        logger.info("‚úÖ Job agendado: Gera√ß√£o de ML Predictions di√°ria √†s 8h")

        # Rotina 7: ML RETRAINING
        # Todo dia √†s 2h da manh√£ (aprende com resultados GREEN/RED)
        self.scheduler.add_job(
            self.ml_retraining_job,
            CronTrigger(hour=2, minute=0),
            id='ml_retraining',
            name='ML Retraining (aprendizado cont√≠nuo)',
            replace_existing=True
        )
        logger.info("‚úÖ Job agendado: ML Retraining di√°rio √†s 2h")

        # Rotina 8: AI AGENT BATCH ANALYSIS
        # A cada 2 horas (refina predictions com contexto)
        self.scheduler.add_job(
            self.ai_agent_batch_job,
            IntervalTrigger(hours=2),
            id='ai_agent_batch',
            name='AI Agent Batch Analysis (100/batch)',
            replace_existing=True
        )
        logger.info("‚úÖ Job agendado: AI Agent batch a cada 2h")

        # Iniciar scheduler
        self.scheduler.start()
        self.is_running = True

        logger.info("üéâ Data Scheduler iniciado com sucesso!")

    def stop(self):
        """Parar scheduler"""
        if not self.is_running:
            return

        logger.info("‚èπÔ∏è Parando Data Scheduler...")
        self.scheduler.shutdown()
        self.is_running = False
        logger.info("‚úÖ Scheduler parado")

    async def daily_incremental_job(self):
        """
        JOB DI√ÅRIO: Coleta incremental

        Executado √†s 6h da manh√£
        Coleta novos jogos e atualiza resultados
        """
        logger.info("üåÖ Executando coleta di√°ria incremental...")

        try:
            # Verificar sa√∫de da quota primeiro
            health = self.quota_manager.check_health()

            if health['status'] == 'CRITICAL':
                logger.error(f"‚ùå Quota cr√≠tica! Abortando coleta di√°ria: {health['message']}")
                return

            # Coletar dados de ontem (resultados finais)
            yesterday = (datetime.now() - timedelta(days=1)).date().isoformat()
            result_yesterday = await self.pipeline.collect_daily_incremental(yesterday)

            logger.info(f"‚úÖ Ontem: {result_yesterday}")

            # Coletar dados de hoje (novos jogos)
            result_today = await self.pipeline.collect_daily_incremental()

            logger.info(f"‚úÖ Hoje: {result_today}")

            # Coletar dados de amanh√£ (pr√≥ximos jogos)
            tomorrow = (datetime.now() + timedelta(days=1)).date().isoformat()
            result_tomorrow = await self.pipeline.collect_daily_incremental(tomorrow)

            logger.info(f"‚úÖ Amanh√£: {result_tomorrow}")

            logger.info("üéâ Coleta di√°ria conclu√≠da com sucesso!")

        except Exception as e:
            logger.error(f"‚ùå Erro na coleta di√°ria: {e}")

    async def live_matches_update_job(self):
        """
        JOB AO VIVO: Atualizar jogos em andamento

        Executado a cada 10 minutos
        Atualiza apenas jogos LIVE para economizar requests
        """
        try:
            # Verificar se h√° jogos ao vivo
            from app.models.api_tracking import FixtureCache

            live_fixtures = self.db.query(FixtureCache).filter(
                FixtureCache.status.in_(['1H', '2H', 'HT', 'LIVE']),
                FixtureCache.needs_update == True
            ).limit(20).all()  # M√°ximo 20 jogos ao vivo

            if not live_fixtures:
                logger.debug("‚ÑπÔ∏è Nenhum jogo ao vivo no momento")
                return

            logger.info(f"‚öΩ Atualizando {len(live_fixtures)} jogos ao vivo...")

            updated = 0

            for cache in live_fixtures:
                # Verificar quota
                if not self.quota_manager.can_make_request(1):
                    logger.warning("‚ö†Ô∏è Quota insuficiente. Parando atualiza√ß√µes ao vivo.")
                    break

                try:
                    # Atualizar fixture
                    fixture = await self.api_service.get_fixture_details(cache.fixture_id)

                    if fixture:
                        cache.raw_fixture_data = fixture
                        cache.status = fixture.get('fixture', {}).get('status', {}).get('short', 'NS')
                        cache.last_synced = datetime.now()

                        # Se finalizou, coletar estat√≠sticas
                        if cache.status == 'FT':
                            cache.needs_update = True

                        updated += 1

                    # Registrar request
                    self.quota_manager.record_request(
                        endpoint='live_update',
                        success=True,
                        results_count=1,
                        params={'fixture': cache.fixture_id}
                    )

                    # Delay adaptativo
                    await asyncio.sleep(self.quota_manager.get_recommended_delay())

                except Exception as e:
                    logger.error(f"Erro ao atualizar fixture {cache.fixture_id}: {e}")

            self.db.commit()

            logger.info(f"‚úÖ {updated} jogos ao vivo atualizados")

        except Exception as e:
            logger.error(f"‚ùå Erro na atualiza√ß√£o ao vivo: {e}")

    async def sync_cache_job(self):
        """
        JOB DE SINCRONIZA√á√ÉO: Cache ‚Üí Database

        Executado a cada hora
        Converte dados brutos do cache em modelos estruturados
        """
        logger.info("üîÑ Executando sincroniza√ß√£o de cache...")

        try:
            result = await self.pipeline.sync_cache_to_database(limit=200)
            logger.info(f"‚úÖ Sync completo: {result}")

        except Exception as e:
            logger.error(f"‚ùå Erro na sincroniza√ß√£o: {e}")

    async def quota_health_check_job(self):
        """
        JOB DE VERIFICA√á√ÉO: Sa√∫de da quota

        Executado a cada 30 minutos
        Monitora uso e alerta se necess√°rio
        """
        try:
            health = self.quota_manager.check_health()

            if health['status'] in ['WARNING', 'CRITICAL']:
                logger.warning(f"‚ö†Ô∏è ALERTA DE QUOTA: {health['message']}")
                logger.warning(f"   A√ß√£o recomendada: {health['recommended_action']}")
            else:
                logger.info(f"‚úÖ Quota saud√°vel: {health['available_requests']} requests dispon√≠veis")

            # Log estat√≠sticas
            stats = self.quota_manager.get_usage_stats()
            logger.info(f"üìä Uso di√°rio: {stats['requests_used']}/{stats['daily_limit']} ({stats['usage_percentage']:.1f}%)")

        except Exception as e:
            logger.error(f"‚ùå Erro na verifica√ß√£o de quota: {e}")

    async def cleanup_cache_job(self):
        """
        JOB DE LIMPEZA: Remover cache antigo

        Executado semanalmente (domingos 3h)
        Remove dados antigos e desnecess√°rios
        """
        logger.info("üßπ Executando limpeza de cache...")

        try:
            from app.models.api_tracking import FixtureCache, APIRequestLog

            # Remover fixtures antigos (mais de 2 anos)
            cutoff_date = datetime.now() - timedelta(days=730)

            old_fixtures = self.db.query(FixtureCache).filter(
                FixtureCache.fixture_date < cutoff_date
            ).delete()

            # Remover logs antigos (mais de 30 dias)
            log_cutoff = datetime.now() - timedelta(days=30)

            old_logs = self.db.query(APIRequestLog).filter(
                APIRequestLog.request_date < log_cutoff
            ).delete()

            self.db.commit()

            logger.info(f"‚úÖ Limpeza conclu√≠da:")
            logger.info(f"   - Fixtures removidos: {old_fixtures}")
            logger.info(f"   - Logs removidos: {old_logs}")

        except Exception as e:
            logger.error(f"‚ùå Erro na limpeza: {e}")

    def get_status(self) -> dict:
        """Obter status do scheduler"""
        if not self.is_running:
            return {'status': 'STOPPED'}

        jobs = []
        for job in self.scheduler.get_jobs():
            jobs.append({
                'id': job.id,
                'name': job.name,
                'next_run': job.next_run_time.isoformat() if job.next_run_time else None
            })

        return {
            'status': 'RUNNING',
            'jobs': jobs,
            'quota_health': self.quota_manager.check_health()
        }

    def ml_predictions_job(self):
        """
        JOB DI√ÅRIO: Gera√ß√£o de ML Predictions

        Executado √†s 8h da manh√£ (ap√≥s coleta de dados √†s 6h)
        Gera 4500 predictions por dia usando ML + Poisson
        """
        logger.info("üß† Executando gera√ß√£o de ML Predictions...")

        try:
            # Gerar 4500 predictions (target ajustado de 2500 para 4500)
            result = run_daily_ml_prediction_generation(target=4500)

            if result.get('success'):
                logger.info(f"‚úÖ ML Predictions geradas: {result.get('predictions_created', 0)} predictions")
                logger.info(f"üìä Singles: {result.get('singles', 0)}, Doubles: {result.get('doubles', 0)}, Trebles: {result.get('trebles', 0)}, Quads: {result.get('quads', 0)}")
            else:
                logger.error(f"‚ùå Erro ao gerar predictions: {result.get('error', 'Unknown')}")

        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico no job de ML predictions: {e}")

    async def ml_retraining_job(self):
        """
        JOB DI√ÅRIO: ML Retraining

        Executado √†s 2h da manh√£ (antes da coleta de dados)
        Retreina modelos com resultados GREEN/RED acumulados
        Melhora accuracy continuamente
        """
        logger.info("ü§ñ Executando ML Retraining...")

        try:
            # Verificar dados suficientes para treino
            from app.models.prediction import Prediction

            results_count = self.db.query(Prediction).filter(
                Prediction.is_winner.isnot(None)  # Tem resultado GREEN/RED
            ).count()

            min_samples = 20  # M√≠nimo de resultados para treinar

            if results_count < min_samples:
                logger.info(f"‚ÑπÔ∏è Dados insuficientes para retraining: {results_count}/{min_samples}")
                return

            logger.info(f"üìä {results_count} resultados dispon√≠veis para treino")

            # Inicializar retraining service
            retraining_service = AutomatedMLRetraining()

            # Retreinar cada modelo
            models_to_train = ['1x2_classifier', 'over_under_classifier', 'btts_classifier']

            for model_name in models_to_train:
                try:
                    trigger = {
                        'trigger_type': 'schedule',
                        'threshold_value': 0.0,
                        'current_value': 0.0,
                        'reason': 'Daily scheduled retraining'
                    }

                    result = retraining_service.retrain_model(model_name, trigger)

                    if result and result.success:
                        improvement = result.improvement * 100
                        logger.info(f"‚úÖ {model_name}: {result.old_accuracy:.1%} ‚Üí {result.new_accuracy:.1%} ({improvement:+.1f}%)")
                    else:
                        logger.warning(f"‚ö†Ô∏è {model_name}: Retraining n√£o melhorou performance")

                except Exception as e:
                    logger.error(f"‚ùå Erro ao retreinar {model_name}: {e}")

            logger.info("üéâ ML Retraining conclu√≠do")

        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico no ML retraining: {e}")

    async def ai_agent_batch_job(self):
        """
        JOB PERI√ìDICO: AI Agent Batch Analysis

        Executado a cada 2 horas
        Analisa predictions pendentes com contexto profundo
        Refina confidence scores e recomenda√ß√µes
        """
        logger.info("üß† Executando AI Agent Batch Analysis...")

        try:
            # Processar 100 predictions pendentes por batch
            result = await analyze_unanalyzed_predictions(limit=100)

            if result.get('success'):
                stats = result.get('stats', {})
                logger.info(f"‚úÖ AI Agent batch conclu√≠do:")
                logger.info(f"   - Processadas: {stats.get('processed', 0)}")
                logger.info(f"   - Sucesso: {stats.get('success', 0)}")
                logger.info(f"   - Falhas: {stats.get('failed', 0)}")
                logger.info(f"   - Puladas: {stats.get('skipped', 0)}")
            else:
                error = result.get('error', 'Unknown')
                logger.error(f"‚ùå AI Agent batch falhou: {error}")

        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico no AI Agent batch: {e}")
