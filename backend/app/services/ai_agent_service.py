"""
üß† AI AGENT SERVICE - An√°lise Contextual Gratuita
Usa Ollama (local) + LangChain para refinar predictions do ML

Stack:
- Ollama (Llama 3.1 8B/70B) - FREE, local
- LangChain - Orchestration
- Zero custo de API
"""
import logging
import json
from typing import Dict, List, Optional
from datetime import datetime

logger = logging.getLogger(__name__)


class AIAgentService:
    """
    AI Agent para an√°lise contextual de predictions
    Usa LLM local (Ollama) sem custos
    """

    def __init__(self, model: str = "llama3.1:8b"):
        """
        Inicializa AI Agent

        Args:
            model: Modelo Ollama (llama3.1:8b, llama3.1:70b, mistral:7b)
        """
        self.model = model
        self.llm = None
        self._initialize_llm()

    def _initialize_llm(self):
        """Inicializa conex√£o com Ollama"""
        try:
            from langchain_community.llms import Ollama

            self.llm = Ollama(
                model=self.model,
                temperature=0.3,  # Baixa temperatura = mais consistente
                num_ctx=4096,     # Contexto grande para an√°lises
            )
            logger.info(f"‚úÖ AI Agent inicializado com modelo: {self.model}")

        except ImportError:
            logger.error("‚ùå langchain_community n√£o instalado. Instale: pip install langchain langchain-community")
            self.llm = None

        except Exception as e:
            logger.error(f"‚ùå Erro ao conectar Ollama: {e}")
            logger.error("üîß Certifique-se que Ollama est√° rodando: ollama serve")
            self.llm = None

    def is_available(self) -> bool:
        """Verifica se AI Agent est√° dispon√≠vel"""
        return self.llm is not None

    def analyze_prediction(
        self,
        match_data: Dict,
        ml_prediction: Dict,
        context_data: Dict,
        few_shot_examples: List[Dict] = None
    ) -> Dict:
        """
        Analisa prediction com contexto profundo

        Args:
            match_data: Dados do jogo (times, liga, data)
            ml_prediction: Prediction do ML (probabilidades, confidence)
            context_data: Contexto externo (not√≠cias, clima, tabela)
            few_shot_examples: Exemplos de GREEN/RED para aprendizado

        Returns:
            An√°lise completa com racioc√≠nio e ajuste de confidence
        """
        if not self.is_available():
            logger.warning("AI Agent n√£o dispon√≠vel, retornando an√°lise b√°sica")
            return self._fallback_analysis(ml_prediction)

        try:
            # Construir prompt com contexto completo
            prompt = self._build_analysis_prompt(
                match_data,
                ml_prediction,
                context_data,
                few_shot_examples
            )

            # Executar an√°lise com LLM
            logger.info(f"üß† Analisando: {match_data.get('home_team')} vs {match_data.get('away_team')}")
            response = self.llm.invoke(prompt)

            # Parsear resposta do LLM
            analysis = self._parse_llm_response(response, ml_prediction)

            logger.info(f"‚úÖ An√°lise conclu√≠da - Confidence ajustada: {analysis['adjusted_confidence']:.2%}")
            return analysis

        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise AI: {e}")
            return self._fallback_analysis(ml_prediction)

    def _build_analysis_prompt(
        self,
        match_data: Dict,
        ml_prediction: Dict,
        context_data: Dict,
        few_shot_examples: List[Dict] = None
    ) -> str:
        """Constr√≥i prompt otimizado para an√°lise"""

        # Few-shot examples (se dispon√≠vel)
        examples_text = ""
        if few_shot_examples:
            examples_text = "\n\nüìö EXEMPLOS DE APRENDIZADO:\n"
            for ex in few_shot_examples[:5]:  # M√°ximo 5 exemplos
                examples_text += f"\n{ex['description']}\nResultado: {ex['outcome']} ({ex['label']})\n"

        # Construir contexto de not√≠cias
        news_text = ""
        if context_data.get('recent_news'):
            news_text = "\n\nüì∞ NOT√çCIAS RECENTES:\n"
            for news in context_data['recent_news'][:3]:
                news_text += f"‚Ä¢ {news}\n"

        prompt = f"""Voc√™ √© um analista profissional de apostas esportivas. Analise esta previs√£o de jogo considerando TODOS os fatores contextuais.

üèüÔ∏è JOGO:
{match_data['home_team']} vs {match_data['away_team']}
Liga: {match_data.get('league', 'N/A')}
Data: {match_data.get('match_date', 'N/A')}

üìä AN√ÅLISE MATEM√ÅTICA (ML):
‚Ä¢ Probabilidades:
  - Casa: {ml_prediction.get('probability_home', 0):.1%}
  - Empate: {ml_prediction.get('probability_draw', 0):.1%}
  - Fora: {ml_prediction.get('probability_away', 0):.1%}
‚Ä¢ Prediction: {ml_prediction.get('predicted_outcome')}
‚Ä¢ Confidence ML: {ml_prediction.get('confidence', 0):.1%}
‚Ä¢ Mercados analisados: {', '.join(ml_prediction.get('markets', ['1X2']))}

üåç CONTEXTO EXTERNO:
‚Ä¢ Rivalidade: {context_data.get('rivalry_level', 'baixa')}
‚Ä¢ Motiva√ß√£o Casa: {context_data.get('motivation_home', 'normal')}
‚Ä¢ Motiva√ß√£o Fora: {context_data.get('motivation_away', 'normal')}
‚Ä¢ Clima: {context_data.get('weather', 'bom')}
‚Ä¢ Les√µes importantes: {', '.join(context_data.get('key_injuries', ['nenhuma']))}
‚Ä¢ Posi√ß√£o tabela (Casa): {context_data.get('home_position', 'N/A')}
‚Ä¢ Posi√ß√£o tabela (Fora): {context_data.get('away_position', 'N/A')}
{news_text}
{examples_text}

üìã SUA TAREFA:

1. ANALISE os fatores contextuais que o ML n√£o consegue ver
2. IDENTIFIQUE fatores que podem aumentar ou diminuir a confidence
3. AJUSTE a confidence do ML baseado no contexto (pode subir ou descer)
4. EXPLIQUE seu racioc√≠nio de forma clara
5. D√ä uma recomenda√ß√£o final (BET / SKIP / MONITOR)

RESPONDA NO FORMATO JSON:

{{
  "context_analysis": "An√°lise detalhada dos fatores contextuais",
  "key_factors": ["fator1", "fator2", "fator3"],
  "confidence_adjustment": 0.72,
  "adjustment_reasoning": "Por que ajustou a confidence",
  "recommendation": "BET",
  "risk_level": "MEDIUM",
  "explanation": "Explica√ß√£o completa para o usu√°rio"
}}

IMPORTANTE:
- Sea confidence ML j√° √© boa E contexto favor√°vel ‚Üí AUMENTE (at√© 0.85 max)
- Se contexto desfavor√°vel (chuva, les√µes chave) ‚Üí DIMINUA
- Se muita incerteza ‚Üí Recomende SKIP
- Seja objetivo e baseado em dados
"""

        return prompt

    def _parse_llm_response(self, response: str, ml_prediction: Dict) -> Dict:
        """
        Parseia resposta do LLM e extrai dados estruturados

        Args:
            response: Resposta em texto do LLM
            ml_prediction: Prediction original do ML

        Returns:
            An√°lise estruturada
        """
        try:
            # Tentar extrair JSON da resposta
            # LLMs √†s vezes adicionam texto antes/depois do JSON
            json_start = response.find('{')
            json_end = response.rfind('}') + 1

            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                parsed = json.loads(json_str)

                # Validar e normalizar
                return {
                    'context_analysis': parsed.get('context_analysis', ''),
                    'key_factors': parsed.get('key_factors', []),
                    'adjusted_confidence': float(parsed.get('confidence_adjustment', ml_prediction.get('confidence', 0.5))),
                    'adjustment_reasoning': parsed.get('adjustment_reasoning', ''),
                    'recommendation': parsed.get('recommendation', 'MONITOR').upper(),
                    'risk_level': parsed.get('risk_level', 'MEDIUM').upper(),
                    'explanation': parsed.get('explanation', ''),
                    'ml_confidence': ml_prediction.get('confidence', 0.5),
                    'confidence_delta': float(parsed.get('confidence_adjustment', 0.5)) - ml_prediction.get('confidence', 0.5)
                }

            else:
                raise ValueError("JSON n√£o encontrado na resposta")

        except Exception as e:
            logger.error(f"Erro ao parsear resposta LLM: {e}")
            logger.debug(f"Resposta original: {response}")

            # Fallback: extrair informa√ß√µes do texto livre
            return {
                'context_analysis': response[:500],  # Primeiros 500 chars
                'key_factors': [],
                'adjusted_confidence': ml_prediction.get('confidence', 0.5),
                'adjustment_reasoning': 'An√°lise textual (sem JSON)',
                'recommendation': 'MONITOR',
                'risk_level': 'MEDIUM',
                'explanation': response,
                'ml_confidence': ml_prediction.get('confidence', 0.5),
                'confidence_delta': 0.0
            }

    def _fallback_analysis(self, ml_prediction: Dict) -> Dict:
        """An√°lise b√°sica quando AI Agent n√£o est√° dispon√≠vel"""
        return {
            'context_analysis': 'AI Agent n√£o dispon√≠vel - usando apenas ML',
            'key_factors': ['An√°lise matem√°tica pura'],
            'adjusted_confidence': ml_prediction.get('confidence', 0.5),
            'adjustment_reasoning': 'Sem an√°lise contextual',
            'recommendation': 'MONITOR',
            'risk_level': 'MEDIUM',
            'explanation': 'Esta prediction foi gerada apenas com ML. Ative o AI Agent para an√°lise contextual.',
            'ml_confidence': ml_prediction.get('confidence', 0.5),
            'confidence_delta': 0.0,
            'ai_available': False
        }

    def batch_analyze(
        self,
        predictions: List[Dict],
        context_data_list: List[Dict],
        max_concurrent: int = 5
    ) -> List[Dict]:
        """
        Analisa m√∫ltiplas predictions em batch

        Args:
            predictions: Lista de predictions com match_data e ml_prediction
            context_data_list: Lista de contextos correspondentes
            max_concurrent: M√°ximo de an√°lises simult√¢neas

        Returns:
            Lista de an√°lises
        """
        if not self.is_available():
            logger.warning("AI Agent n√£o dispon√≠vel para batch analysis")
            return [self._fallback_analysis(p['ml_prediction']) for p in predictions]

        results = []

        # Processar em lotes para n√£o sobrecarregar
        for i in range(0, len(predictions), max_concurrent):
            batch = predictions[i:i + max_concurrent]
            batch_contexts = context_data_list[i:i + max_concurrent]

            for pred, context in zip(batch, batch_contexts):
                analysis = self.analyze_prediction(
                    match_data=pred['match_data'],
                    ml_prediction=pred['ml_prediction'],
                    context_data=context
                )
                results.append(analysis)

        logger.info(f"‚úÖ Batch analysis conclu√≠da: {len(results)} predictions analisadas")
        return results
